{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from os import listdir\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
      "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\n",
      "1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\n",
      "1001773457_577c3a7d70.jpg#0\tA black dog and a spotted dog are fighting\n",
      "1001773457_577c3a7d70.jpg#1\tA black dog and a tri-colored dog playing with each other on the road .\n",
      "1001773457_577c3a7d70.jpg#2\tA black dog and a white dog with brown spots are staring at each other in the street .\n",
      "1001773457_577c3a7d70.jpg#3\tTwo dogs of different breeds looking at each other on the road .\n",
      "1001773457_577c3a7d70.jpg#4\tTwo dogs on pavement moving toward each other .\n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    with open(filename) as file:\n",
    "        text = file.readlines()\n",
    "        return text\n",
    "\n",
    "filename = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "text = load_doc(filename)\n",
    "for line in text[:10]:\n",
    "    print(line,end='')\n",
    "\n",
    "# every image has 5 captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping image with captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "A girl going into a wooden building .\n",
      "A little girl climbing into a wooden playhouse .\n",
      "A little girl climbing the stairs to her playhouse .\n",
      "A little girl in a pink dress going into a wooden cabin .\n"
     ]
    }
   ],
   "source": [
    "def image_to_captions(text):\n",
    "    hash_map = {}\n",
    "    for line in text:\n",
    "        token = line.split()\n",
    "        image_id = token[0].split('.')[0] # separating with '.' to extract image id (removing .jpg)\n",
    "        image_caption = ' '.join(token[1: ])\n",
    "        \n",
    "        if(image_id not in hash_map):\n",
    "            hash_map[image_id] = [image_caption]\n",
    "        else:\n",
    "            hash_map[image_id].append(image_caption)\n",
    "        \n",
    "    return hash_map\n",
    "        \n",
    "\n",
    "map_img_to_captions = image_to_captions(text)\n",
    "print(*map_img_to_captions['1000268201_693b08cb0e'],sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child in pink dress is climbing up set of stairs in an entry way',\n",
       " 'girl going into wooden building',\n",
       " 'little girl climbing into wooden playhouse',\n",
       " 'little girl climbing the stairs to her playhouse',\n",
       " 'little girl in pink dress going into wooden cabin']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(map_img_to_captions):\n",
    "    preprocessed_captions = []\n",
    "    for key in map_img_to_captions.keys():\n",
    "        for idx in range(len(map_img_to_captions[key])):\n",
    "            tokens = map_img_to_captions[key][idx].split()\n",
    "            tokens = [token.lower() for token in tokens if len(token)>1 if token.isalpha()]\n",
    "            map_img_to_captions[key][idx] = ' '.join(tokens)\n",
    "            \n",
    "    return map_img_to_captions\n",
    "\n",
    "\n",
    "preprocessed_map = preprocess(map_img_to_captions)\n",
    "preprocessed_map['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabulary from captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 8357\n"
     ]
    }
   ],
   "source": [
    "def create_vocabulary(preprocessed_map):\n",
    "    vocabulary = set()\n",
    "    for img_captions in preprocessed_map.values(): # list of 5 captions for each image\n",
    "        for caption in img_captions:\n",
    "            for token in caption.split():\n",
    "                vocabulary.add(token)    \n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "vocabulary = create_vocabulary(preprocessed_map)\n",
    "print('Vocabulary size',len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving image id and preprocessed captions to new text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_captions(preprocessed_map,filename):\n",
    "    data = []\n",
    "    for image_id,image_captions in preprocessed_map.items():\n",
    "        for caption in image_captions:\n",
    "            data.append(image_id + ' ' + caption + '\\n')\n",
    "            \n",
    "    with open(filename,'w') as file:\n",
    "        for line in data:\n",
    "            file.write(line)\n",
    "\n",
    "save_captions(preprocessed_map,'preprocessed_captions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images (image name) of Train & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2513260012_03d33305cf\n",
      "2903617548_d3e38d7f88\n",
      "3338291921_fe7ae0c8f8\n",
      "488416045_1c6d903fe0\n",
      "2644326817_8f45080b87\n",
      "\n",
      "Number of images in train data 6000\n",
      "\n",
      "Number of images in train data 1000\n"
     ]
    }
   ],
   "source": [
    "def img_id_train(filename):\n",
    "    with open(filename) as file:\n",
    "        data = file.readlines()\n",
    "        train_img_name = []\n",
    "        for img_id in data:\n",
    "            train_img_name.append(img_id.split('.')[0])\n",
    "    return train_img_name    \n",
    "\n",
    "train_img_name = img_id_train('Flickr8k_text/Flickr_8k.trainImages.txt')\n",
    "test_img_name  = img_id_train('Flickr8k_text/Flickr_8k.testImages.txt')\n",
    "print(*train_img_name[:5],sep='\\n')\n",
    "print('\\nNumber of images in train data',len(train_img_name))\n",
    "print('\\nNumber of images in train data',len(test_img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2513260012_03d33305cf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_name[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-processed captions for Train data (add 'startseq' & 'endseq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq child in pink dress is climbing up set of stairs in an entry way endseq\n",
      "startseq girl going into wooden building endseq\n",
      "startseq little girl climbing into wooden playhouse endseq\n",
      "startseq little girl climbing the stairs to her playhouse endseq\n",
      "startseq little girl in pink dress going into wooden cabin endseq\n"
     ]
    }
   ],
   "source": [
    "def load_captions_train(filename):\n",
    "    doc = load_doc(filename) \n",
    "    train_captions = {}    \n",
    "    \n",
    "    for line in doc:\n",
    "        tokens = line.split()\n",
    "        image_id, image_caption = tokens[0], tokens[1:]\n",
    "\n",
    "        if(image_id in train_img_name):\n",
    "            if(image_id not in train_captions):\n",
    "                train_captions[image_id] = []\n",
    "            \n",
    "            modified_caption = 'startseq ' + ' '.join(image_caption) + ' endseq'\n",
    "            train_captions[image_id].append(modified_caption)\n",
    "    \n",
    "    return train_captions\n",
    "\n",
    "\n",
    "train_captions = load_captions_train('preprocessed_captions.txt')\n",
    "print(*train_captions['1000268201_693b08cb0e'],sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path,target_size=(299,299)) \n",
    "    # type(img): PIL.Image.Image\n",
    "#     plt.figure(figsize=(12,6))\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(img)\n",
    "#     plt.title('Original Image(Resized)')\n",
    "\n",
    "    img = image.img_to_array(img) # Converts PIL Image instance to numpy array (299,299,3)\n",
    "    img = np.expand_dims(img, axis=0) #Add one more dimension: (1, 299, 299, 3) # Inception-V3 requires 4 dimensions\n",
    "    img = preprocess_input(img) # preprocess image as per Inception-V3 model\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(img[0])\n",
    "#     plt.title('Preprocessed image for Inception-V3')    \n",
    "    \n",
    "    return img  # shape: (1, 299, 299, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Inception-V3 model\n",
    "model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# Create new model, by removing last layer (output layer) from Inception-V3\n",
    "model_new = Model(inputs=model.input, outputs=model.layers[-2].output) # outputs=(second last layer output)\n",
    "\n",
    "# model.layers            list of layers\n",
    "# model.input             <tf.Tensor 'input_1:0' shape=(?, ?, ?, 3) dtype=float32>\n",
    "# model.layers[-1].output <tf.Tensor 'predictions/Softmax:0' shape=(?, 1000) dtype=float32> 1000 target classes\n",
    "# model.layers[-2].output <tf.Tensor 'avg_pool/Mean:0' shape=(?, 2048) dtype=float32>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All images names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_name(path):\n",
    "    img_name = set([path+image for image in listdir(path)])\n",
    "    return img_name\n",
    "\n",
    "path = 'Flicker8k_Dataset/'\n",
    "all_images_name = images_name(path)\n",
    "train_img_name = [path+img+'.jpg' for img in train_img_name]\n",
    "test_img_name = [path+img+'.jpg' for img in test_img_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode images into feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images encoded  500\n",
      "Train images encoded  1000\n",
      "Train images encoded  1500\n",
      "Train images encoded  2000\n",
      "Train images encoded  2500\n",
      "Train images encoded  3000\n",
      "Train images encoded  3500\n",
      "Train images encoded  4000\n",
      "Train images encoded  4500\n",
      "Train images encoded  5000\n",
      "Train images encoded  5500\n",
      "Train images encoded  6000\n",
      "** Time taken for encoding train images 888.4454591274261 seconds **\n",
      "Test images encoded  200\n",
      "Test images encoded  400\n",
      "Test images encoded  600\n",
      "Test images encoded  800\n",
      "Test images encoded  1000\n",
      "** Time taken for encoding test images 139.03138494491577 seconds **\n"
     ]
    }
   ],
   "source": [
    "# Function to encode given image into a vector of size (2048, )\n",
    "def encode_image(image):\n",
    "    image = preprocess_image(image) # preprocess image\n",
    "    feature_vector = model_new.predict(image) # Get encoding vector for image\n",
    "    feature_vector = feature_vector.reshape(feature_vector.shape[1], ) # reshape from (1, 2048) to (2048, )\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "# To encode all train images\n",
    "start_train = time()\n",
    "encoding_train = {}\n",
    "for idx,img in enumerate(train_img_name):\n",
    "    if( (idx+1)%500 == 0):\n",
    "        print('Train images encoded ',idx+1)\n",
    "    encoding_train[img] = encode_image(img)\n",
    "print(\"** Time taken for encoding train images {} seconds **\".format(time()-start_train))\n",
    "\n",
    "\n",
    "# To encode all test images\n",
    "start_test = time()\n",
    "encoding_test = {}\n",
    "for idx,img in enumerate(test_img_name):\n",
    "    if( (idx+1)%200 == 0):\n",
    "        print('Test images encoded ',idx+1)\n",
    "    encoding_test[img] = encode_image(img)\n",
    "print(\"** Time taken for encoding test images {} seconds **\".format(time()-start_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the learned features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the bottleneck train features to disk\n",
    "with open(\"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    dump(encoding_train, encoded_pickle)\n",
    "    \n",
    "# # Save the bottleneck test features to disk\n",
    "with open(\"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
    "     dump(encoding_test, encoded_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All train captions (6000 x 5 = 30,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "startseq child in pink dress is climbing up set of stairs in an entry way endseq\n",
      "startseq girl going into wooden building endseq\n",
      "startseq little girl climbing into wooden playhouse endseq\n",
      "startseq little girl climbing the stairs to her playhouse endseq\n",
      "startseq little girl in pink dress going into wooden cabin endseq\n",
      "startseq black dog and spotted dog are fighting endseq\n",
      "startseq black dog and dog playing with each other on the road endseq\n",
      "startseq black dog and white dog with brown spots are staring at each other in the street endseq\n",
      "startseq two dogs of different breeds looking at each other on the road endseq\n",
      "startseq two dogs on pavement moving toward each other endseq\n"
     ]
    }
   ],
   "source": [
    "train_features = load(open(\"encoded_train_images.pkl\", \"rb\"))\n",
    "\n",
    "# Create a list of all the training captions\n",
    "all_train_captions = []\n",
    "for captions in train_captions.values():\n",
    "    for caption in captions:\n",
    "        all_train_captions.append(caption)\n",
    "    \n",
    "print(len(all_train_captions))\n",
    "print(*all_train_captions[:10],sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider only those words which occur atleast 10 times in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original tokens 7265\n",
      "Number of tokens after threshold 1643\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for caption in all_train_captions:\n",
    "    for token in caption.split():\n",
    "        corpus.append(token)\n",
    "        \n",
    "hash_map = Counter(corpus)\n",
    "vocab = []\n",
    "for token,count in hash_map.items():\n",
    "    if(count>=10):\n",
    "        vocab.append(token)\n",
    "        \n",
    "print('Number of original tokens',len(hash_map))\n",
    "print('Number of tokens after threshold',len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping: Index to words & Word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1643\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "    \n",
    "for idx,token in enumerate(vocab):\n",
    "    word_to_index[token] = idx+1\n",
    "    index_to_word[idx+1] = token\n",
    "\n",
    "vocab_size = len(index_to_word) + 1 # one for appended 0's\n",
    "\n",
    "print(len(index_to_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate max-length caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of caption=  33\n"
     ]
    }
   ],
   "source": [
    "def max_len_caption(all_train_captions):   \n",
    "    max_len = 0\n",
    "    for caption in all_train_captions:\n",
    "        max_len = max(max_len,len(caption.split()))\n",
    "    print('Maximum length of caption= ',max_len)\n",
    "    return max_len\n",
    "\n",
    "max_length_caption = max_len_caption(all_train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation using Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            temp='Flickr8k_Dataset/Flicker8k_Dataset/'\n",
    "            \n",
    "            photo = photos[temp+key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {} # empty dictionary\n",
    "f = open('glove.6B.200d.txt', encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1644, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_to_index.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 33, 200)      328800      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 33, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          467968      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1644)         422508      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,809,612\n",
      "Trainable params: 1,809,612\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length_caption,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.embeddings.Embedding object at 0x000001F8A745DA08>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[2])\n",
    "\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robust_loss_pytorch.general\n",
    "import keras.backend as K\n",
    "adaptive = robust_loss_pytorch.adaptive.AdaptiveLossFunction(num_dims = 2, float_dtype=np.float32, device='cpu')\n",
    "#loss = torch.mean(adaptive.lossfun((y_i - y)[:,None]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss\n",
    "#def custom_loss(layer):\n",
    "\n",
    "    # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
    "    #def loss(y_true,y_pred):\n",
    "        #return K.mean(K.square(y_pred - y_true) + K.square(layer), axis=-1)\n",
    "   \n",
    "    # Return a function\n",
    "    #return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "custom_loss() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-b31bc98d7a14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\zidan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zidan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m#                   layer losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# Functions for train, test and predict will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zidan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[1;34m(self, masks)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                     output_loss = loss_fn(\n\u001b[1;32m--> 692\u001b[1;33m                         y_true, y_pred, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zidan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mscope_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lambda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'<lambda>'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             return losses_utils.compute_weighted_loss(\n\u001b[0;32m     73\u001b[0m                 losses, sample_weight, reduction=self.reduction)\n",
      "\u001b[1;32mc:\\users\\zidan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mLoss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: custom_loss() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "model.compile(loss=custom_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "number_pics_per_bath = 3\n",
    "steps = len(train_captions)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp='Flickr8k_Dataset/Flicker8k_Dataset/'\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(epochs):\n",
    "#     generator = data_generator(train_captions, train_features, word_to_index, max_length_caption, number_pics_per_bath)\n",
    "#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "#     model.save('./model_weights/model_' + str(i) + '.h5')\n",
    "Image.open('training_1.PNG')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(epochs):\n",
    "#     generator = data_generator(train_captions, train_features, word_to_index, max_length_caption, number_pics_per_bath)\n",
    "#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "#     model.save('./model_weights/model_' + str(i) + '.h5')\n",
    "Image.open('training_2.PNG')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "epochs = 10\n",
    "number_pics_per_bath = 6\n",
    "steps = len(train_captions)//number_pics_per_bath\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     generator = data_generator(train_captions, train_features, word_to_index, max_length_caption, number_pics_per_bath)\n",
    "#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "#     #model.save('./model_weights/model_' + str(i) + '.h5')\n",
    "\n",
    "Image.open('training_3.PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./model_weights/model_30.h5')\n",
    "# model.load_weights('./model_weights/model_30.h5')\n",
    "\n",
    "images = 'Flicker8k_Dataset/'\n",
    "with open(\"encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoding_test = load(encoded_pickle)\n",
    "    \n",
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z+=1\n",
    "# pic = list(encoding_test.keys())[z]\n",
    "# image = encoding_test[pic].reshape((1,2048))\n",
    "# x=plt.imread(images+pic)\n",
    "# plt.imshow(x)\n",
    "# plt.show()\n",
    "# print(\"Greedy:\",greedySearch(image))\n",
    "\n",
    "Image.open('inference.PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__=='__main__':\n",
    "    \n",
    "#     # load all captions \n",
    "#     text = load_doc(\"Flickr8k_text/Flickr8k.token.txt\") # list\n",
    "#     for line in text[:10]:\n",
    "#         print(line,end='')\n",
    "    \n",
    "#     # map image to captions in dictionary \n",
    "#     map_img_to_captions = image_to_captions(text) # dictionary (key:img name) (value-list of 5 captions)\n",
    "#     print(*map_img_to_captions['1000268201_693b08cb0e'],sep='\\n')\n",
    "    \n",
    "#     # preprocess captions\n",
    "#     preprocessed_map = preprocess(map_img_to_captions) # dictionary (key:img name) (value-list of 5 preprocessed-captions)\n",
    "#     print(preprocessed_map['1000268201_693b08cb0e'])\n",
    "\n",
    "#     # prepare vocabulary from captions\n",
    "#     vocabulary = create_vocabulary(preprocessed_map) # list\n",
    "#     print('Vocabulary size',len(vocabulary))\n",
    "    \n",
    "#     # save image name with preprocessed captions in a new text file\n",
    "#     filename = 'preprocessed_captions.txt'\n",
    "#     save_captions(preprocessed_map,filename)\n",
    "#     print('Saved image names with preprocessed captions in {}'.format(filename))\n",
    "    \n",
    "    \n",
    "#     # load image name for train & test data\n",
    "#     train_img_name = img_id_train('Flickr8k_text/Flickr_8k.trainImages.txt') # list\n",
    "#     test_img_name  = img_id_train('Flickr8k_text/Flickr_8k.testImages.txt')  # list \n",
    "#     print('\\nNumber of images in train data',len(train_img_name)) \n",
    "#     print('\\nNumber of images in train data',len(test_img_name))\n",
    "    \n",
    "#     # load captions for train data and 'start_seq' and 'end_seq' in starting and end of each caption respectively\n",
    "#     train_captions = load_captions_train('preprocessed_captions.txt')\n",
    "#     print(*train_captions['1000268201_693b08cb0e'],sep='\\n')\n",
    "    \n",
    "    \n",
    "#     # image names for complete, train and test data\n",
    "#     path = 'Flickr8k_Dataset/Flicker8k_Dataset/'\n",
    "#     all_images_name = images_name(path)\n",
    "#     train_img_name = [path+img+'.jpg' for img in train_img_name]\n",
    "#     test_img_name = [path+img+'.jpg' for img in test_img_name]\n",
    "    \n",
    "    \n",
    "#     # Load Inception-V3 model\n",
    "#     model = InceptionV3(weights='imagenet')\n",
    "#     # Create new model, by removing last layer (output layer) from Inception-V3\n",
    "#     model_new = Model(inputs=model.input, outputs=model.layers[-2].output) # outputs=(second last layer output)\n",
    "    \n",
    "    \n",
    "# #     # encode all train images\n",
    "# #     start_train = time()\n",
    "# #     encoding_train = {}\n",
    "# #     for idx,img in enumerate(train_img_name):\n",
    "# #         if( (idx+1)%500 == 0):\n",
    "# #             print('Train images encoded ',idx+1)\n",
    "# #         encoding_train[img] = encode_image(img)\n",
    "# #     print(\"** Time taken for encoding train images {} seconds **\".format(time()-start_train))\n",
    "\n",
    "\n",
    "# #     # encode all test images\n",
    "# #     start_test = time()\n",
    "# #     encoding_test = {}\n",
    "# #     for idx,img in enumerate(test_img_name):\n",
    "# #         if( (idx+1)%200 == 0):\n",
    "# #             print('Test images encoded ',idx+1)\n",
    "# #         encoding_test[img] = encode_image(img)\n",
    "# #     print(\"** Time taken for encoding test images {} seconds **\".format(time()-start_test))\n",
    "    \n",
    "    \n",
    "# # Save the bottleneck train features to disk\n",
    "# #     with open(\"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
    "# #         dump(encoding_train, encoded_pickle)\n",
    "\n",
    "# # Save the bottleneck test features to disk\n",
    "# #     with open(\"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
    "# #         dump(encoding_test, encoded_pickle)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
